{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIET Codescrum Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Generation using Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this workshops is to show an exciting application of recurrent neural networks. These types of neural networks excel at processing natural language and creating sequential models for __NLP applications__. The RNNs can also be used for __machine translation__, __speech recognition__, __sentiment analysis__, __DNA sequences analysis__, etc. We hope to spark your curiosity so that you can play with them and think novel ways to apply neural networks in your fields of interest.\n",
    "\n",
    "Deep Learning and in particular RNNs can also be applied to Telecommunications, there are some articles about __call volume forecasting__ using RNNs, __learning interconnection networks structures__ and __modelling server workloads__ using time series data and RNNs sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning](./images/deep_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: __learn by example__. Deep Learning is based on one of the early machine learning algorithms: __Artificial Neural Networks__.\n",
    "\n",
    "Neural Networks are inspired by our understanding of the biology of our brains – all those interconnections between the neurons. But, unlike a biological brain where any neuron can connect to any other neuron within a certain physical distance, these artificial neural networks have __discrete layers__, __connections__, and __directions of data propagation__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Artificial Neural Networks for Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of Neural Networks](./images/types_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are a lot of types of Artificial Neural Networks, each of them devised with a particular purpose. For example the Standard NN are very useful at classic __regression and classification__ tasks using structured data. However, Convolutional NN are best suited for working with unstructured visual data such as images or video for tasks like __image recognition__, __tagging__, etc. The Recurrent NN are used mostly for __Natural Language Processing__ and __sequential data__ (time series, music, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Regression analysis_ estimates the relationship between statistical input variables in order to predict an outcome variable. Logistic regression is a regression model that uses input variables to predict a categorical outcome variable that can take on one of a limited set of class values, for example “__cancer__” / “__no cancer__”.\n",
    "\n",
    "Logistic regression applies the logistic sigmoid function to weighted input values to generate a prediction of which of two classes the input data belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid Function](./images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is similar to a non-linear __perceptron__ or a neural network without _hidden layers_. The main difference from other basic models is that logistic regression is easy to interpret and reliable if some statistical properties for the input variables hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function takes in weighted data (matrix multiplication between input data and weights) and outputs a non-linear transformation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activation Functions](./images/activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Cost Function (Error Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _Machine Learning_, cost functions are used to estimate how badly models are performing. Put simply, __a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y__. This cost function (you may also see this referred to as loss or error.) can be estimated by iteratively running the model to compare estimated predictions against “ground truth” — the known values of y.\n",
    "\n",
    "The objective of a ML model, therefore, is to find parameters, weights or a structure that __minimises__ the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Log Loss](./images/log_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a technique to minimize __loss__ by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of __weights__ and bias to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent](./images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Gradient Descent__ Variants\n",
    "    - __Batch__ gradient descent\n",
    "    - __Stochastic__ gradient descent\n",
    "    - __Mini-batch__ gradient descent\n",
    "* __RMSProp__\n",
    "* __ADAM__\n",
    "* __Momentum__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of RNNs](./images/rnn_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sequential processing in absence of sequences__. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to process them in a sequential manner. For instance, the figure below shows results from two very nice papers from [DeepMind](http://deepmind.com/). On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right ([Ba et al.](http://arxiv.org/abs/1412.7755)). On the right, a recurrent network generates images of digits by learning to sequentially add color to a canvas ([Gregor et al.](http://arxiv.org/abs/1502.04623)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/house_read.gif\" alt=\"drawing\" width=\"300px\"/> <img src=\"./images/house_generate.gif\" alt=\"drawing\" width=\"400px\" height=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see a RNN in action. This workshop is based on the _Classical-Piano-Composer_ Github Repo by [Skuldur](https://github.com/Skuldur) (Sigurður Skúli Sigurgeirsson). To generate music using RNNs one of the most important steps is the audio pre-processing, meaning transforming the songs that we are going to use to train the model into a appropriate format for a RNN (i.e. In a sequence of notes and chords).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Notes from the music files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the RNN we are going to use music files in an special format. The most convenient format to work with is the __midi__ format. This format is used to describe electronic instruments musical features likes notes, chords, duration, tempo, etc. In this case the music files consists from piano songs from video games like Final Fantasy and The Legend Of Zelda.\n",
    "\n",
    "Using the __music21__ python package developed by the _MIT_ we can extract the musical features we need to represent the songs as a list of strings representing the notes and chords. Running the following cell will parse the songs and then append them to a list called notes. The songs must be in _midi_ format and be located in the __midi_songs__ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs/0fithos.mid\n",
      "Parsing midi_songs/8.mid\n",
      "Parsing midi_songs/ahead_on_our_way_piano.mid\n",
      "Parsing midi_songs/AT.mid\n",
      "Parsing midi_songs/balamb.mid\n",
      "Parsing midi_songs/bcm.mid\n",
      "Parsing midi_songs/BlueStone_LastDungeon.mid\n",
      "Parsing midi_songs/braska.mid\n",
      "Parsing midi_songs/caitsith.mid\n",
      "Parsing midi_songs/Cids.mid\n",
      "Parsing midi_songs/cosmo.mid\n",
      "Parsing midi_songs/costadsol.mid\n",
      "Parsing midi_songs/dayafter.mid\n",
      "Parsing midi_songs/decisive.mid\n",
      "Parsing midi_songs/dontbeafraid.mid\n",
      "Parsing midi_songs/DOS.mid\n",
      "Parsing midi_songs/electric_de_chocobo.mid\n",
      "Parsing midi_songs/Eternal_Harvest.mid\n",
      "Parsing midi_songs/EyesOnMePiano.mid\n",
      "Parsing midi_songs/ff11_awakening_piano.mid\n",
      "Parsing midi_songs/ff1battp.mid\n",
      "Parsing midi_songs/FF3_Battle_(Piano).mid\n",
      "Parsing midi_songs/FF3_Third_Phase_Final_(Piano).mid\n",
      "Parsing midi_songs/ff4-airship.mid\n",
      "Parsing midi_songs/Ff4-BattleLust.mid\n",
      "Parsing midi_songs/ff4-fight1.mid\n",
      "Parsing midi_songs/ff4-town.mid\n",
      "Parsing midi_songs/FF4.mid\n",
      "Parsing midi_songs/ff4_piano_collections-main_theme.mid\n",
      "Parsing midi_songs/ff4pclov.mid\n",
      "Parsing midi_songs/FF6epitaph_piano.mid\n",
      "Parsing midi_songs/ff6shap.mid\n",
      "Parsing midi_songs/Ff7-Cinco.mid\n",
      "Parsing midi_songs/Ff7-Jenova_Absolute.mid\n",
      "Parsing midi_songs/ff7-mainmidi.mid\n",
      "Parsing midi_songs/Ff7-One_Winged.mid\n",
      "Parsing midi_songs/ff7themep.mid\n",
      "Parsing midi_songs/ff8-lfp.mid\n",
      "Parsing midi_songs/FF8_Shuffle_or_boogie_pc.mid\n",
      "Parsing midi_songs/FFIII_Edgar_And_Sabin_Piano.mid\n",
      "Parsing midi_songs/FFIX_Piano.mid\n",
      "Parsing midi_songs/FFIXQuMarshP.mid\n",
      "Parsing midi_songs/FFVII_BATTLE.mid\n",
      "Parsing midi_songs/FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
      "Parsing midi_songs/Fiend_Battle_(Piano).mid\n",
      "Parsing midi_songs/Fierce_Battle_(Piano).mid\n",
      "Parsing midi_songs/figaro.mid\n",
      "Parsing midi_songs/Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
      "Parsing midi_songs/Final_Fantasy_Matouyas_Cave_Piano.mid\n",
      "Parsing midi_songs/Finalfantasy5gilgameshp.mid\n",
      "Parsing midi_songs/Finalfantasy6fanfarecomplete.mid\n",
      "Parsing midi_songs/fortresscondor.mid\n",
      "Parsing midi_songs/Fyw_piano.mid\n",
      "Parsing midi_songs/gerudo.mid\n",
      "Parsing midi_songs/Gold_Silver_Rival_Battle.mid\n",
      "Parsing midi_songs/goldsaucer.mid\n",
      "Parsing midi_songs/great_war.mid\n",
      "Parsing midi_songs/HighwindTakestotheSkies.mid\n",
      "Parsing midi_songs/In_Zanarkand.mid\n",
      "Parsing midi_songs/JENOVA.mid\n",
      "Parsing midi_songs/Kingdom_Hearts_Dearly_Beloved.mid\n",
      "Parsing midi_songs/Kingdom_Hearts_Traverse_Town.mid\n",
      "Parsing midi_songs/Life_Stream.mid\n",
      "Parsing midi_songs/lurk_in_dark.mid\n",
      "Parsing midi_songs/mining.mid\n",
      "Parsing midi_songs/Oppressed.mid\n",
      "Parsing midi_songs/OTD5YA.mid\n",
      "Parsing midi_songs/path_of_repentance.mid\n",
      "Parsing midi_songs/pkelite4.mid\n",
      "Parsing midi_songs/Rachel_Piano_tempofix.mid\n",
      "Parsing midi_songs/redwings.mid\n",
      "Parsing midi_songs/relmstheme-piano.mid\n",
      "Parsing midi_songs/roseofmay-piano.mid\n",
      "Parsing midi_songs/rufus.mid\n",
      "Parsing midi_songs/Rydia_pc.mid\n",
      "Parsing midi_songs/sandy.mid\n",
      "Parsing midi_songs/sera_.mid\n",
      "Parsing midi_songs/sobf.mid\n",
      "Parsing midi_songs/Still_Alive-1.mid\n",
      "Parsing midi_songs/Suteki_Da_Ne_(Piano_Version).mid\n",
      "Parsing midi_songs/thenightmarebegins.mid\n",
      "Parsing midi_songs/thoughts.mid\n",
      "Parsing midi_songs/tifap.mid\n",
      "Parsing midi_songs/tpirtsd-piano.mid\n",
      "Parsing midi_songs/traitor.mid\n",
      "Parsing midi_songs/ultimafro.mid\n",
      "Parsing midi_songs/ultros.mid\n",
      "Parsing midi_songs/VincentPiano.mid\n",
      "Parsing midi_songs/ViviinAlexandria.mid\n",
      "Parsing midi_songs/waltz_de_choco.mid\n",
      "Parsing midi_songs/z_aeristhemepiano.mid\n",
      "Parsing midi_songs/Zelda_Overworld.mid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59020"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import music21\n",
    "from lstm import *\n",
    "\n",
    "notes = get_notes()\n",
    "len(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The songs get splitted into two object types: __Notes__ and __Chords__. Note objects contain information about the pitch, octave, and offset of the Note.\n",
    "\n",
    "* __Pitch__ refers to the frequency of the sound, or how high or low it is and is represented with the letters [A, B, C, D, E, F, G], with A being the highest and G being the lowest.\n",
    "\n",
    "* __Octave__ refers to which set of pitches you use on a piano.\n",
    "\n",
    "* __Offset__ refers to where the note is located in the piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C5', 'A4', 'C5', 'A4', '6', 'C5', 'E5', 'F#5', 'E5', '5', 'C5', 'A4', 'C5', 'A4', '4', 'C5', 'E5', 'F5', 'E5', '5', 'C5', 'A4', 'C5', 'A4', '6', 'C5', 'E5', 'F#5', 'E5', '5', 'C5', 'A4', 'C5', 'A4', 'A2', 'C5', 'E5', 'F5', 'A2', 'E5', 'A2', 'C5', 'B4', 'C5', 'A4', 'A2', 'F4', 'E4', 'F4', 'A2']\n"
     ]
    }
   ],
   "source": [
    "print(notes[200:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Music Sheet Representation](./images/music.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that to generate music accurately our neural network will have to be able to predict which note or chord is next. That means that our prediction array will have to contain every note and chord object that we encounter in our training set. \n",
    "\n",
    "Next, we will create a mapping function to map from string-based categorical data to integer-based numerical data. This is done because neural network perform much better with integer-based numerical data than string-based categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Categorical Encoding](./images/categorical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to generate the input and output sequences for the model. In this case the __sequence_lenght__ was set to 100. This means that the RNN uses the previous 100 notes to predict the next note. The final task in this step is to normalize and encode the outputs to input them to the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN architecture used to train the model is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN Architecture](./images/rnn_architecture_workshop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM__ stands for __Long Short Term Memory__ this is a special Recurrent Neural Network that has the ability to learn long range connections in a sequence by using a \"_memory cell_\" that can store a relation betwen elements in a sequence. This helps to solve the problem of __vanishing gradients__ that is very common in deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vanishing Gradients](./images/vanishing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dropout__ is a regularisation technique that \"shutdown\" some of the layer inputs according to a probability. This helps to prevent __overfitting__ improving the generalization power of the input/output mapping of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dropout](./images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layer (Fully Connected Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a __Dense__ layer each neuron is connected to every neuron in the previous layer, and each connection has it's own weight. This is layer is usually used with the output of a non-linear activation from the previous layer and is also used before the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dense](./images/dense.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Softmax](./images/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented RNN architecture is then coded using __Keras__ and __Tensorflow__. Keras allows to easily construct Neural Networks using pre defined high level abstractions of the components mentioned before.\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses __Categorical Cross Entropy__ as _Loss Function_ and the __RMSProp__ algorithm as the optimizer, the optimizing algorithm can be chosed empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Categorical Cross Entropy](./images/cat_log_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model, for this task we must define the last training parameters. Which are the __epochs__ and __batch size__. The number of epochs mean the number of times the training dataset is passed through the network and then the parameters are optimized. The batch size is used for the optimization algorithm. The recommended batch size is usually a power of 2 less than or equal to 64. If we set the batch size to 1 then the optimization algorithm becomes __Stochastic Gradient Descent__.\n",
    "\n",
    "```\n",
    "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training the RNN](./images/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Music!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the musical sequence require the reverse steps. This is converting the predicted sequence to notes and converting theses notes to a midi file to generate the final song. To generate the predictions we select a random ponit in the sequence and then start predicting what is the next note. The program is hard coded to predict the next 500 notes.\n",
    "\n",
    "Calling the __generate()__ method will use the same network that we trained, load the parameters and process the sequences to generate a _midi_ file called __test_output.midi__. In the following cell we generate the song and print the details about the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 359)               92263     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 359)               0         \n",
      "=================================================================\n",
      "Total params: 5,474,663\n",
      "Trainable params: 5,474,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from predict import *\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi2audio import FluidSynth\n",
    "import IPython.display as ipd\n",
    "\n",
    "FluidSynth().midi_to_audio('test_output.mid', 'output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "ipd.Audio('output.wav') # load a local WAV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://developer.nvidia.com/deep-learning\n",
    "* https://www.mathworks.com/discovery/deep-learning.html\n",
    "* [Coursera Deep Learning Specialization, Andrew Ng](https://www.deeplearning.ai/)\n",
    "* https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
    "* https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220\n",
    "* https://developers.google.com/machine-learning/crash-course/glossary\n",
    "* https://www.quora.com/Does-Gradient-Descent-Algo-always-converge-to-the-global-minimum\n",
    "* http://ruder.io/optimizing-gradient-descent/index.html\n",
    "* https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
